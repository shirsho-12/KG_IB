{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90a56795",
   "metadata": {},
   "source": [
    "## Semantic Relation Clustering - IB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d157f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Callable, Dict, Iterable, List, Optional, Sequence, Tuple, Set\n",
    "import math\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pathlib import Path\n",
    "import openai\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "load_dotenv()\n",
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "OPENROUTER_BASE_URL = os.getenv(\"OPENROUTER_BASE_URL\", \"https://openrouter.ai/api/v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b7b790be",
   "metadata": {},
   "outputs": [],
   "source": [
    "Triple = Tuple[str, str, str]          # (head, relation, tail)\n",
    "TypePair = Tuple[str, str]             # (type(head), type(tail))\n",
    "EmbeddingFn = Callable[[str, str, str, str], np.ndarray]\n",
    "TypeFn = Callable[[str], str]\n",
    "TripleExtractorFn = Callable[[str, int], List[Triple]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3947e72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_path = Path.cwd() / \"output\" / \"webnlg\" / \"triplets.txt\"\n",
    "data_path = Path.cwd() / \"data\" / \"webnlg.txt\"\n",
    "\n",
    "triplets_text = triplet_path.read_text().splitlines()\n",
    "\n",
    "import ast\n",
    "all_triplets = [ast.literal_eval(line) for line in triplets_text]\n",
    "\n",
    "from model.openai_model import OpenAIModel\n",
    "from agent.core_agent import Agent\n",
    "\n",
    "model = OpenAIModel(\n",
    "    model_name=\"openai/gpt-4o-mini\",\n",
    "    base_url=OPENROUTER_BASE_URL,\n",
    "    api_key=OPENROUTER_API_KEY,\n",
    "    temperature=0.0\n",
    ")\n",
    "type_function_prompt = \"\"\"\n",
    "You are an expert at classifying entities into types. Given an entity, return its type in one or two words. Be concise and specific.\n",
    "Examples:\n",
    "- \"Barack Obama\" -> \"Person\"\n",
    "- \"New York City\" -> \"Location\"\n",
    "\n",
    "Only return the type without any additional explanation.\n",
    "Input: \"{entity}\"\n",
    "Output:\n",
    "\"\"\"\n",
    "\n",
    "type_function_agent = Agent(\n",
    "    llm=model,\n",
    "    prompt=type_function_prompt\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a793ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from typing import Union\n",
    "\n",
    "\n",
    "def default_triple_extractor(sentence: str, idx=0) -> List[Triple]:\n",
    "    \"\"\"\n",
    "    Stub: Extract (head, relation, tail) triples from a sentence.\n",
    "    Replace with your actual IE model (OpenIE, SRL, custom, etc.).\n",
    "\n",
    "    For now, returns [] so nothing happens unless you swap it out.\n",
    "    \"\"\"\n",
    "    return all_triplets[idx]\n",
    "\n",
    "\n",
    "def type_function(entity: str) -> str:\n",
    "    \"\"\"\n",
    "    GPT-based type function.\n",
    "    \"\"\"\n",
    "    return type_function_agent.run({\"entity\": entity}).strip().upper()\n",
    "\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    api_key=OPENROUTER_API_KEY,\n",
    "    base_url=OPENROUTER_BASE_URL\n",
    ")\n",
    "\n",
    "\n",
    "def embedding_fn(head: str, relation: str, tail: str, triple_type: Union[str, Tuple[str, str]]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    GPT-based embedding function.\n",
    "    \"\"\"\n",
    "    if isinstance(triple_type, tuple):\n",
    "        triple_type = f\"{triple_type[0]}->{triple_type[1]}\"\n",
    "    text = f\"{head} {relation} {tail} [{triple_type}]\"\n",
    "    response = client.embeddings.create(\n",
    "        input=[text],\n",
    "        model=\"text-embedding-3-small\"\n",
    "    )\n",
    "    embeddings = [\n",
    "        torch.tensor(data_point.embedding) for data_point in response.data\n",
    "        ]\n",
    "    return torch.stack(embeddings).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fcac2d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class RelationCluster:\n",
    "    \"\"\"\n",
    "    Represents an induced relation type.\n",
    "\n",
    "    Maintains:\n",
    "    - mean embedding μ\n",
    "    - diagonal variance estimate σ^2 (via online Welford updates)\n",
    "    - multinomial over argument type pairs (type(head), type(tail))\n",
    "    - set of surface relation strings that landed here\n",
    "    \"\"\"\n",
    "    # running stats for embeddings\n",
    "    mean: np.ndarray\n",
    "    var_diag: np.ndarray\n",
    "    count: int\n",
    "\n",
    "    # argument-type distribution\n",
    "    type_counts: Dict[TypePair, int] = field(default_factory=dict)\n",
    "\n",
    "    # surface labels\n",
    "    surface_relations: set = field(default_factory=set)\n",
    "\n",
    "    # small constant for numerical stability\n",
    "    eps: float = 1e-6\n",
    "\n",
    "    @classmethod\n",
    "    def from_first_example(\n",
    "        cls,\n",
    "        head: str,\n",
    "        relation: str,\n",
    "        tail: str,\n",
    "        emb: np.ndarray,\n",
    "        type_pair: TypePair,\n",
    "    ) -> \"RelationCluster\":\n",
    "        return cls(\n",
    "            mean=emb.copy(),\n",
    "            var_diag=np.ones_like(emb, dtype=np.float32),  # initial variance guess\n",
    "            count=1,\n",
    "            type_counts={type_pair: 1},\n",
    "            surface_relations={relation},\n",
    "        )\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        head: str,\n",
    "        relation: str,\n",
    "        tail: str,\n",
    "        emb: np.ndarray,\n",
    "        type_pair: TypePair,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Online update of:\n",
    "        - mean and diag variance (Welford-like per-dimension)\n",
    "        - type_counts\n",
    "        - surface_relations\n",
    "        \"\"\"\n",
    "        self.surface_relations.add(relation)\n",
    "\n",
    "        # Welford update for diagonal covariance\n",
    "        self.count += 1\n",
    "        delta = emb - self.mean\n",
    "        # update mean\n",
    "        self.mean += delta / float(self.count)\n",
    "        # recompute delta to new mean\n",
    "        delta2 = emb - self.mean\n",
    "        # online update of diagonal variance estimate\n",
    "        # M2_diag = (count-1)*var_diag; var_new = M2_new / (count-1)\n",
    "        M2_diag = self.var_diag * (self.count - 2)  # previous count-1 = current-2\n",
    "        M2_diag += delta * delta2\n",
    "        if self.count > 1:\n",
    "            self.var_diag = M2_diag / float(self.count - 1)\n",
    "        else:\n",
    "            self.var_diag = np.ones_like(self.mean, dtype=np.float32)\n",
    "\n",
    "        # keep some minimum variance\n",
    "        self.var_diag = np.maximum(self.var_diag, self.eps)\n",
    "\n",
    "        # update type distribution\n",
    "        self.type_counts[type_pair] = self.type_counts.get(type_pair, 0) + 1\n",
    "\n",
    "    # ---------- type distortion (argument-role compatibility) ----------\n",
    "\n",
    "    def type_probability(\n",
    "        self,\n",
    "        type_pair: TypePair,\n",
    "        all_type_pairs: Sequence[TypePair],\n",
    "        alpha_smooth: float = 1.0,\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Laplace-smoothed probability P(type_pair | cluster).\n",
    "        \"\"\"\n",
    "        K = len(all_type_pairs)\n",
    "        total = sum(self.type_counts.values())\n",
    "        count = self.type_counts.get(type_pair, 0)\n",
    "        return (count + alpha_smooth) / (total + alpha_smooth * K)\n",
    "\n",
    "    def type_distortion(\n",
    "        self,\n",
    "        type_pair: TypePair,\n",
    "        all_type_pairs: Sequence[TypePair],\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        D_type = -log2 P(type_pair | cluster).\n",
    "        Lower is better.\n",
    "        \"\"\"\n",
    "        p = self.type_probability(type_pair, all_type_pairs)\n",
    "        return -math.log(p + 1e-12, 2.0)  # bits\n",
    "\n",
    "    # ---------- semantic distortion (embedding distance / KL-ish) ----------\n",
    "\n",
    "    def semantic_distortion(self, emb: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Mahalanobis-like distance between emb and cluster Gaussian:\n",
    "\n",
    "        D_sem ≈ (x - μ)^T diag(1/σ^2) (x - μ)\n",
    "\n",
    "        This is proportional to KL(N(μ,σ^2) || N(x, σ0^2 I)) under an isotropic\n",
    "        assumption, so it's a reasonable proxy for semantic KL.\n",
    "        \"\"\"\n",
    "        diff = emb - self.mean\n",
    "        inv_var = 1.0 / (self.var_diag + self.eps)\n",
    "        return float(np.sum(diff * diff * inv_var))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f3189d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Online relation clusterer\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "class RelationClusterer:\n",
    "    \"\"\"\n",
    "    Online clustering of relation instances into induced relation types.\n",
    "\n",
    "    For each triple (h, r, t) in a sentence:\n",
    "      - Compute an embedding embedding_fn(h, r, t, sentence)\n",
    "      - Infer types T1 = type_fn(h), T2 = type_fn(t)\n",
    "      - For each cluster, compute:\n",
    "            cost = w_sem * D_sem + w_type * D_type\n",
    "        where:\n",
    "            D_sem  = semantic_distortion(embedding)\n",
    "            D_type = -log P((T1,T2) | cluster)\n",
    "      - If min_cluster_cost < lambda_new:\n",
    "            assign to that cluster and update its stats\n",
    "        else:\n",
    "            create a new cluster\n",
    "\n",
    "    This is a greedy, streaming approximation to an IB-style objective:\n",
    "        minimize E[cost] + lambda_new * (#clusters)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_fn: EmbeddingFn,\n",
    "        triple_extractor: TripleExtractorFn = default_triple_extractor,\n",
    "        type_fn: TypeFn = type_function,\n",
    "        w_sem: float = 0.5,\n",
    "        w_type: float = 1.0,\n",
    "        lambda_new: float = 3.0,\n",
    "    ) -> None:\n",
    "        self.embedding_fn = embedding_fn\n",
    "        self.triple_extractor = triple_extractor\n",
    "        self.type_fn = type_fn\n",
    "\n",
    "        self.w_sem = w_sem\n",
    "        self.w_type = w_type\n",
    "        self.lambda_new = lambda_new\n",
    "\n",
    "        self.clusters: List[RelationCluster] = []\n",
    "        self.all_type_pairs: Dict[TypePair, None] = {}  # use as ordered set\n",
    "\n",
    "    # ---------- main entry point ----------\n",
    "\n",
    "    def process_sentences(self, sentences: Sequence[str]) -> None:\n",
    "        \"\"\"\n",
    "        Process a sequence of sentences, updating clusters online.\n",
    "        \"\"\"\n",
    "        for idx, sent in tqdm(enumerate(sentences), total=len(sentences)):\n",
    "            triples = self.triple_extractor(sent, idx)\n",
    "            for h, r, t in triples:\n",
    "                self._process_triple(sent, h, r, t)\n",
    "\n",
    "    # ---------- internals ----------\n",
    "\n",
    "    def _process_triple(self, sentence: str, head: str, relation: str, tail: str) -> None:\n",
    "        # 1) embedding\n",
    "        emb = self.embedding_fn(head, relation, tail, sentence)\n",
    "\n",
    "        # 2) argument types\n",
    "        t1 = self.type_fn(head)\n",
    "        t2 = self.type_fn(tail)\n",
    "        type_pair: TypePair = (t1, t2)\n",
    "        if type_pair not in self.all_type_pairs:\n",
    "            self.all_type_pairs[type_pair] = None\n",
    "\n",
    "        # 3) compute cost for each existing cluster\n",
    "        best_idx: Optional[int] = None\n",
    "        best_cost: float = float(\"inf\")\n",
    "        best_Dsem: Optional[float] = None\n",
    "        best_Dtype: Optional[float] = None\n",
    "\n",
    "        type_pair_list = list(self.all_type_pairs.keys())\n",
    "\n",
    "        for idx, cluster in enumerate(self.clusters):\n",
    "            d_sem = cluster.semantic_distortion(emb)\n",
    "            d_type = cluster.type_distortion(type_pair, type_pair_list)\n",
    "            cost = self.w_sem * d_sem + self.w_type * d_type\n",
    "\n",
    "            if cost < best_cost:\n",
    "                best_cost = cost\n",
    "                best_idx = idx\n",
    "                best_Dsem = d_sem\n",
    "                best_Dtype = d_type\n",
    "\n",
    "        # 4) decide: assign vs create new\n",
    "        if best_idx is not None and best_cost < self.lambda_new:\n",
    "            cluster = self.clusters[best_idx]\n",
    "            cluster.update(head, relation, tail, emb, type_pair)\n",
    "            # could log best_Dsem/best_Dtype here if desired\n",
    "        else:\n",
    "            new_cluster = RelationCluster.from_first_example(\n",
    "                head=head,\n",
    "                relation=relation,\n",
    "                tail=tail,\n",
    "                emb=emb,\n",
    "                type_pair=type_pair,\n",
    "            )\n",
    "            self.clusters.append(new_cluster)\n",
    "\n",
    "    # ---------- convenience methods / inspection ----------\n",
    "\n",
    "    def get_clusters_summary(self) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Return a light-weight summary of clusters for inspection.\n",
    "        \"\"\"\n",
    "        summaries = []\n",
    "        for idx, c in enumerate(self.clusters):\n",
    "            summaries.append(\n",
    "                {\n",
    "                    \"cluster_id\": idx,\n",
    "                    \"surface_relations\": sorted(c.surface_relations),\n",
    "                    \"n_triples\": c.count,\n",
    "                    \"type_counts\": dict(c.type_counts),\n",
    "                }\n",
    "            )\n",
    "        return summaries\n",
    "\n",
    "    def print_clusters(self, max_width: int = 120) -> None:\n",
    "        \"\"\"\n",
    "        Pretty-print clusters in a compact way.\n",
    "        \"\"\"\n",
    "        import textwrap\n",
    "        for summary in self.get_clusters_summary():\n",
    "            s = textwrap.shorten(str(summary), width=max_width)\n",
    "            print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7744dcf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1165 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▊  | 916/1165 [2:57:09<48:09, 11.60s/it]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      1\u001b[39m sentences = Path(data_path).read_text().splitlines()\n\u001b[32m      3\u001b[39m clusterer = RelationClusterer(\n\u001b[32m      4\u001b[39m     embedding_fn=embedding_fn,\n\u001b[32m      5\u001b[39m     triple_extractor=default_triple_extractor,\n\u001b[32m   (...)\u001b[39m\u001b[32m      9\u001b[39m     lambda_new=\u001b[32m3.0\u001b[39m,\n\u001b[32m     10\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43mclusterer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess_sentences\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m clusterer.print_clusters()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 55\u001b[39m, in \u001b[36mRelationClusterer.process_sentences\u001b[39m\u001b[34m(self, sentences)\u001b[39m\n\u001b[32m     53\u001b[39m triples = \u001b[38;5;28mself\u001b[39m.triple_extractor(sent, idx)\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m h, r, t \u001b[38;5;129;01min\u001b[39;00m triples:\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_triple\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 65\u001b[39m, in \u001b[36mRelationClusterer._process_triple\u001b[39m\u001b[34m(self, sentence, head, relation, tail)\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# 2) argument types\u001b[39;00m\n\u001b[32m     64\u001b[39m t1 = \u001b[38;5;28mself\u001b[39m.type_fn(head)\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m t2 = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtype_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtail\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m type_pair: TypePair = (t1, t2)\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m type_pair \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.all_type_pairs:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mtype_function\u001b[39m\u001b[34m(entity)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtype_function\u001b[39m(entity: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m     12\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[33;03m    GPT-based type function.\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtype_function_agent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mentity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mentity\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m.strip().upper()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/Practice/agent/core_agent.py:20\u001b[39m, in \u001b[36mAgent.run\u001b[39m\u001b[34m(self, invoke_dct)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Generates output based on the input dictionary.\"\"\"\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43minvoke_dct\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/graph_of/lib/python3.11/site-packages/langchain_core/runnables/base.py:3246\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3244\u001b[39m                 input_ = context.run(step.invoke, input_, config, **kwargs)\n\u001b[32m   3245\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3246\u001b[39m                 input_ = context.run(step.invoke, input_, config)\n\u001b[32m   3247\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3248\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/Practice/model/openai_model.py:32\u001b[39m, in \u001b[36mOpenAIModel.invoke\u001b[39m\u001b[34m(self, prompt, *args, **kwargs)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[33;03mInvoke the OpenAI model with a prompt.\u001b[39;00m\n\u001b[32m     22\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     28\u001b[39m \u001b[33;03m    The model's response as a string.\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     31\u001b[39m     \u001b[38;5;66;03m# self.logger.info(\"Invoking OpenAI model...\")\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     35\u001b[39m     \u001b[38;5;66;03m# self.logger.error(f\"Error invoking OpenAI model: {str(e)}\", exc_info=True)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/graph_of/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:395\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    383\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    384\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    385\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    390\u001b[39m     **kwargs: Any,\n\u001b[32m    391\u001b[39m ) -> BaseMessage:\n\u001b[32m    392\u001b[39m     config = ensure_config(config)\n\u001b[32m    393\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    394\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    405\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/graph_of/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:1025\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1016\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1017\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1018\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     **kwargs: Any,\n\u001b[32m   1023\u001b[39m ) -> LLMResult:\n\u001b[32m   1024\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1025\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/graph_of/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:842\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    840\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    841\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m842\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    844\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    846\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    847\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    848\u001b[39m         )\n\u001b[32m    849\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    850\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/graph_of/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:1091\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1089\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1090\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1091\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1092\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1093\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1094\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1095\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/graph_of/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:1208\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1201\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _construct_lc_result_from_responses_api(\n\u001b[32m   1202\u001b[39m             response,\n\u001b[32m   1203\u001b[39m             schema=original_schema_obj,\n\u001b[32m   1204\u001b[39m             metadata=generation_info,\n\u001b[32m   1205\u001b[39m             output_version=\u001b[38;5;28mself\u001b[39m.output_version,\n\u001b[32m   1206\u001b[39m         )\n\u001b[32m   1207\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1208\u001b[39m         raw_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_raw_response\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1209\u001b[39m         response = raw_response.parse()\n\u001b[32m   1210\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/graph_of/lib/python3.11/site-packages/openai/_legacy_response.py:364\u001b[39m, in \u001b[36mto_raw_response_wrapper.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    360\u001b[39m extra_headers[RAW_RESPONSE_HEADER] = \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    362\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mextra_headers\u001b[39m\u001b[33m\"\u001b[39m] = extra_headers\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/graph_of/lib/python3.11/site-packages/openai/_utils/_utils.py:286\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/graph_of/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py:1156\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1110\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1111\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1112\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1153\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   1154\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1155\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1156\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1157\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1158\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1159\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1160\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1161\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1162\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1163\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1164\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1165\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1166\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1167\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1168\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1170\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1171\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1173\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1174\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1175\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1176\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1177\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1178\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1179\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1180\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1181\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1184\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1185\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1186\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1187\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1188\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1189\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1190\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1191\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1192\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1198\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1199\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1200\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1201\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1202\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1203\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1204\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1205\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/graph_of/lib/python3.11/site-packages/openai/_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/graph_of/lib/python3.11/site-packages/openai/_base_client.py:982\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m    980\u001b[39m response = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    981\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m982\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    987\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    988\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered httpx.TimeoutException\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/graph_of/lib/python3.11/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/graph_of/lib/python3.11/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/graph_of/lib/python3.11/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/graph_of/lib/python3.11/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/graph_of/lib/python3.11/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/graph_of/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/graph_of/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/graph_of/lib/python3.11/site-packages/httpcore/_sync/connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/graph_of/lib/python3.11/site-packages/httpcore/_sync/http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/graph_of/lib/python3.11/site-packages/httpcore/_sync/http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/graph_of/lib/python3.11/site-packages/httpcore/_sync/http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/graph_of/lib/python3.11/site-packages/httpcore/_sync/http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/graph_of/lib/python3.11/site-packages/httpcore/_backends/sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/graph_of/lib/python3.11/ssl.py:1295\u001b[39m, in \u001b[36mSSLSocket.recv\u001b[39m\u001b[34m(self, buflen, flags)\u001b[39m\n\u001b[32m   1291\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1292\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1293\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1294\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1295\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1296\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1297\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv(buflen, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/graph_of/lib/python3.11/ssl.py:1168\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1166\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[32m   1167\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1168\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[32m   1170\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.args[\u001b[32m0\u001b[39m] == SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.suppress_ragged_eofs:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "sentences = Path(data_path).read_text().splitlines()\n",
    "\n",
    "clusterer = RelationClusterer(\n",
    "    embedding_fn=embedding_fn,\n",
    "    triple_extractor=default_triple_extractor,\n",
    "    type_fn=type_function,\n",
    "    w_sem=0.5,\n",
    "    w_type=1.0,\n",
    "    lambda_new=3.0,\n",
    ")\n",
    "\n",
    "clusterer.process_sentences(sentences)\n",
    "clusterer.print_clusters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b253714b",
   "metadata": {},
   "source": [
    "## Online Clusterer - Pragmatic Equivalence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbb8626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MI computation utility\n",
    "def mutual_information_binary(N11, N10, N01, N00):\n",
    "    \"\"\"\n",
    "    Compute mutual information I(X;Y) for binary events:\n",
    "        X = \"edge of cluster c1 under mapping M\"\n",
    "        Y = \"edge of cluster c2 under mapping M\"\n",
    "\n",
    "    Where:\n",
    "      N11 = count(X=1, Y=1)\n",
    "      N10 = count(X=1, Y=0)\n",
    "      N01 = count(X=0, Y=1)\n",
    "      N00 = count(X=0, Y=0)\n",
    "\n",
    "    Returns MI in bits.\n",
    "    \"\"\"\n",
    "    N = N11 + N10 + N01 + N00\n",
    "    if N == 0:\n",
    "        return 0.0\n",
    "\n",
    "    P11 = N11 / N\n",
    "    P10 = N10 / N\n",
    "    P01 = N01 / N\n",
    "    P00 = N00 / N\n",
    "\n",
    "    PX1 = P11 + P10\n",
    "    PY1 = P11 + P01\n",
    "    PX0 = 1 - PX1\n",
    "    PY0 = 1 - PY1\n",
    "\n",
    "    MI = 0.0\n",
    "\n",
    "    def add_term(Pxy, Px, Py):\n",
    "        return Pxy * math.log2(Pxy / (Px * Py)) if Pxy > 0 else 0.0\n",
    "\n",
    "    MI += add_term(P11, PX1, PY1)\n",
    "    MI += add_term(P10, PX1, PY0)\n",
    "    MI += add_term(P01, PX0, PY1)\n",
    "    MI += add_term(P00, PX0, PY0)\n",
    "\n",
    "    return MI\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RelationClusterView:\n",
    "    \"\"\"\n",
    "    Lightweight extensional view:\n",
    "    - cluster_id\n",
    "    - canonical type_pair\n",
    "    - edges: set of (h, t)\n",
    "    \"\"\"\n",
    "    cluster_id: int\n",
    "    type_pair: TypePair\n",
    "    edges: Set[Tuple[str, str]] = field(default_factory=set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e5f5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RelationCluster:\n",
    "    \"\"\"\n",
    "    Represents an induced relation type.\n",
    "\n",
    "    Maintains:\n",
    "    - mean embedding μ and diagonal variance σ^2 (online)\n",
    "    - type_counts over (type(head), type(tail))\n",
    "    - surface_relations: set of raw relation strings\n",
    "    - cluster_id: int assigned by clusterer\n",
    "    \"\"\"\n",
    "    cluster_id: int\n",
    "    mean: np.ndarray\n",
    "    var_diag: np.ndarray\n",
    "    count: int\n",
    "\n",
    "    type_counts: Dict[TypePair, int] = field(default_factory=dict)\n",
    "    surface_relations: Set[str] = field(default_factory=set)\n",
    "    eps: float = 1e-6\n",
    "\n",
    "    @classmethod\n",
    "    def from_first_example(\n",
    "        cls,\n",
    "        cluster_id: int,\n",
    "        head: str,\n",
    "        relation: str,\n",
    "        tail: str,\n",
    "        emb: np.ndarray,\n",
    "        type_pair: TypePair,\n",
    "    ) -> \"RelationCluster\":\n",
    "        return cls(\n",
    "            cluster_id=cluster_id,\n",
    "            mean=emb.copy(),\n",
    "            var_diag=np.ones_like(emb, dtype=np.float32),\n",
    "            count=1,\n",
    "            type_counts={type_pair: 1},\n",
    "            surface_relations={relation},\n",
    "        )\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        head: str,\n",
    "        relation: str,\n",
    "        tail: str,\n",
    "        emb: np.ndarray,\n",
    "        type_pair: TypePair,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Online update of:\n",
    "        - mean & diag variance via Welford per-dimension\n",
    "        - type_counts\n",
    "        - surface_relations\n",
    "        \"\"\"\n",
    "        self.surface_relations.add(relation)\n",
    "\n",
    "        self.count += 1\n",
    "        delta = emb - self.mean\n",
    "        self.mean += delta / float(self.count)\n",
    "        delta2 = emb - self.mean\n",
    "\n",
    "        # Online diag variance: M2_diag / (count-1) = var_diag\n",
    "        M2_diag = self.var_diag * (self.count - 2)  # previous (count-1) = current-2\n",
    "        M2_diag += delta * delta2\n",
    "        if self.count > 1:\n",
    "            self.var_diag = M2_diag / float(self.count - 1)\n",
    "        else:\n",
    "            self.var_diag = np.ones_like(self.mean, dtype=np.float32)\n",
    "\n",
    "        self.var_diag = np.maximum(self.var_diag, self.eps)\n",
    "\n",
    "        self.type_counts[type_pair] = self.type_counts.get(type_pair, 0) + 1\n",
    "\n",
    "    # ---- type distortion (argument-role compatibility) ----\n",
    "\n",
    "    def canonical_type_pair(self) -> TypePair:\n",
    "        \"\"\"\n",
    "        Return the most frequent type_pair (for MI / equivalence).\n",
    "        \"\"\"\n",
    "        if not self.type_counts:\n",
    "            return (\"UNKNOWN\", \"UNKNOWN\")\n",
    "        return max(self.type_counts.items(), key=lambda kv: kv[1])[0]\n",
    "\n",
    "    def type_probability(\n",
    "        self,\n",
    "        type_pair: TypePair,\n",
    "        all_type_pairs: Sequence[TypePair],\n",
    "        alpha_smooth: float = 1.0,\n",
    "    ) -> float:\n",
    "        K = len(all_type_pairs)\n",
    "        total = sum(self.type_counts.values())\n",
    "        count = self.type_counts.get(type_pair, 0)\n",
    "        return (count + alpha_smooth) / (total + alpha_smooth * K)\n",
    "\n",
    "    def type_distortion(\n",
    "        self,\n",
    "        type_pair: TypePair,\n",
    "        all_type_pairs: Sequence[TypePair],\n",
    "    ) -> float:\n",
    "        \"\"\"D_type = -log2 P(type_pair | cluster). Lower is better.\"\"\"\n",
    "        p = self.type_probability(type_pair, all_type_pairs)\n",
    "        return -math.log(p + 1e-12, 2.0)\n",
    "\n",
    "    # ---- semantic distortion (embedding distance / KL-ish) ----\n",
    "\n",
    "    def semantic_distortion(self, emb: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Mahalanobis-like distance:\n",
    "        D_sem ≈ (x - μ)^T diag(1/σ^2) (x - μ)\n",
    "        \"\"\"\n",
    "        diff = emb - self.mean\n",
    "        inv_var = 1.0 / (self.var_diag + self.eps)\n",
    "        return float(np.sum(diff * diff * inv_var))\n",
    "\n",
    "\n",
    "class PragmaticEquivalenceLearner:\n",
    "    \"\"\"\n",
    "    Learns pragmatic equivalence (same-direction or inverse) between\n",
    "    induced relation clusters using extensional mutual information.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mi_threshold: float = 0.25, min_shared_pairs: int = 2) -> None:\n",
    "        self.mi_threshold = mi_threshold\n",
    "        self.min_shared_pairs = min_shared_pairs\n",
    "\n",
    "        self.views: Dict[int, RelationClusterView] = {}\n",
    "        self.equivalence_classes: Dict[int, Set[int]] = defaultdict(set)\n",
    "        self.inverse_map: Dict[int, int] = {}  # cluster_id -> inverse_cluster_id\n",
    "\n",
    "    def ingest(self, clusters: List[RelationCluster], fact_list: List[Tuple[str, str, str, int]]) -> None:\n",
    "        \"\"\"\n",
    "        Build RelationClusterView objects from clusters and facts.\n",
    "        \"\"\"\n",
    "        for c in clusters:\n",
    "            self.views[c.cluster_id] = RelationClusterView(\n",
    "                cluster_id=c.cluster_id,\n",
    "                type_pair=c.canonical_type_pair(),\n",
    "            )\n",
    "\n",
    "        for (h, r, t, cid) in fact_list:\n",
    "            if cid in self.views:\n",
    "                self.views[cid].edges.add((h, t))\n",
    "\n",
    "    def compute_equivalences(self) -> None:\n",
    "        ids = sorted(self.views.keys())\n",
    "        for i in range(len(ids)):\n",
    "            for j in range(i + 1, len(ids)):\n",
    "                v1 = self.views[ids[i]]\n",
    "                v2 = self.views[ids[j]]\n",
    "\n",
    "                same_dir = (v1.type_pair == v2.type_pair)\n",
    "                inverse_dir = (v1.type_pair == (v2.type_pair[1], v2.type_pair[0]))\n",
    "\n",
    "                if not (same_dir or inverse_dir):\n",
    "                    continue\n",
    "\n",
    "                if same_dir:\n",
    "                    MI = self._mi_same_direction(v1, v2)\n",
    "                    direction = \"same\"\n",
    "                else:\n",
    "                    MI = self._mi_inverse_direction(v1, v2)\n",
    "                    direction = \"inverse\"\n",
    "\n",
    "                if MI <= 0:\n",
    "                    continue\n",
    "\n",
    "                H1 = self._binary_entropy(v1.edges)\n",
    "                H2 = self._binary_entropy(v2.edges)\n",
    "                denom = min(H1, H2) if min(H1, H2) > 0 else 1.0\n",
    "                NMI = MI / denom\n",
    "\n",
    "                if NMI >= self.mi_threshold:\n",
    "                    self.equivalence_classes[v1.cluster_id].add(v2.cluster_id)\n",
    "                    self.equivalence_classes[v2.cluster_id].add(v1.cluster_id)\n",
    "                    if direction == \"inverse\":\n",
    "                        self.inverse_map[v1.cluster_id] = v2.cluster_id\n",
    "                        self.inverse_map[v2.cluster_id] = v1.cluster_id\n",
    "\n",
    "    def _mi_same_direction(self, v1: RelationClusterView, v2: RelationClusterView) -> float:\n",
    "        all_pairs = v1.edges.union(v2.edges)\n",
    "        if len(all_pairs) < self.min_shared_pairs:\n",
    "            return 0.0\n",
    "        e1 = v1.edges\n",
    "        e2 = v2.edges\n",
    "\n",
    "        N11 = sum(1 for p in all_pairs if (p in e1 and p in e2))\n",
    "        N10 = sum(1 for p in all_pairs if (p in e1 and p not in e2))\n",
    "        N01 = sum(1 for p in all_pairs if (p in e2 and p not in e1))\n",
    "        N00 = len(all_pairs) - (N11 + N10 + N01)\n",
    "        return mutual_information_binary(N11, N10, N01, N00)\n",
    "\n",
    "    def _mi_inverse_direction(self, v1: RelationClusterView, v2: RelationClusterView) -> float:\n",
    "        e1 = v1.edges\n",
    "        e2_swapped = {(t, h) for (h, t) in v2.edges}\n",
    "        all_pairs = e1.union(e2_swapped)\n",
    "        if len(all_pairs) < self.min_shared_pairs:\n",
    "            return 0.0\n",
    "\n",
    "        N11 = sum(1 for p in all_pairs if (p in e1 and p in e2_swapped))\n",
    "        N10 = sum(1 for p in all_pairs if (p in e1 and p not in e2_swapped))\n",
    "        N01 = sum(1 for p in all_pairs if (p in e2_swapped and p not in e1))\n",
    "        N00 = len(all_pairs) - (N11 + N10 + N01)\n",
    "        return mutual_information_binary(N11, N10, N01, N00)\n",
    "\n",
    "    @staticmethod\n",
    "    def _binary_entropy(edge_set: Set[Tuple[str, str]]) -> float:\n",
    "        N = len(edge_set)\n",
    "        if N == 0:\n",
    "            return 0.0\n",
    "        P1 = N / (N + 1e-12)\n",
    "        P0 = 1 - P1\n",
    "        return -(P1 * math.log2(P1 + 1e-12) + P0 * math.log2(P0 + 1e-12))\n",
    "\n",
    "\n",
    "############################################################\n",
    "# Redundancy checker based on equivalence classes\n",
    "############################################################\n",
    "\n",
    "class PragmaticRedundancyChecker:\n",
    "    \"\"\"\n",
    "    Uses learned pragmatic equivalence (same + inverse) to decide\n",
    "    whether a new triple (h, cid, t) is redundant.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, learner: PragmaticEquivalenceLearner) -> None:\n",
    "        self.learner = learner\n",
    "        self.forward_edges = defaultdict(set)   # (h, cid) -> set(t)\n",
    "        self.backward_edges = defaultdict(set)  # (t, cid) -> set(h)\n",
    "\n",
    "    def add_fact(self, h: str, cid: int, t: str) -> None:\n",
    "        self.forward_edges[(h, cid)].add(t)\n",
    "        self.backward_edges[(t, cid)].add(h)\n",
    "\n",
    "    def is_redundant(self, h: str, cid: int, t: str) -> bool:\n",
    "        # Direct same-cluster fact\n",
    "        if t in self.forward_edges.get((h, cid), set()):\n",
    "            return True\n",
    "\n",
    "        # Check equivalent clusters\n",
    "        eq_class = self.learner.equivalence_classes.get(cid, set())\n",
    "\n",
    "        for cid2 in eq_class:\n",
    "            # same direction\n",
    "            if t in self.forward_edges.get((h, cid2), set()):\n",
    "                return True\n",
    "\n",
    "            # inverse direction\n",
    "            inv = self.learner.inverse_map.get(cid2)\n",
    "            if inv is not None and inv == cid:\n",
    "                if h in self.backward_edges.get((t, cid2), set()):\n",
    "                    return True\n",
    "\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c74c305",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class OnlineRelationClusterer:\n",
    "    \"\"\"\n",
    "    Streaming clustering of relation instances into induced relation types.\n",
    "\n",
    "    For each triple (h,r,t) extracted from sentences:\n",
    "      - compute embedding\n",
    "      - infer types (T1,T2)\n",
    "      - cost(cluster) = w_sem * D_sem + w_type * D_type\n",
    "      - if min cost < lambda_new: assign to cluster, else new cluster.\n",
    "\n",
    "    Stores:\n",
    "      - clusters (RelationCluster)\n",
    "      - fact_list: (head, relation, tail, cluster_id) for all accepted facts\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_fn: EmbeddingFn,\n",
    "        triple_extractor: TripleExtractorFn = default_triple_extractor,\n",
    "        type_fn: TypeFn = type_function,\n",
    "        w_sem: float = 0.5,\n",
    "        w_type: float = 1.0,\n",
    "        lambda_new: float = 3.0,\n",
    "    ) -> None:\n",
    "        self.embedding_fn = embedding_fn\n",
    "        self.triple_extractor = triple_extractor\n",
    "        self.type_fn = type_fn\n",
    "\n",
    "        self.w_sem = w_sem\n",
    "        self.w_type = w_type\n",
    "        self.lambda_new = lambda_new\n",
    "\n",
    "        self.clusters: List[RelationCluster] = []\n",
    "        self.all_type_pairs_ordered: List[TypePair] = []\n",
    "        self._type_pair_index: Dict[TypePair, int] = {}\n",
    "        self.fact_list: List[Tuple[str, str, str, int]] = []  # (h, r, t, cluster_id)\n",
    "\n",
    "    # ---- type-pair registry ----\n",
    "\n",
    "    def _register_type_pair(self, tp: TypePair) -> None:\n",
    "        if tp not in self._type_pair_index:\n",
    "            self._type_pair_index[tp] = len(self.all_type_pairs_ordered)\n",
    "            self.all_type_pairs_ordered.append(tp)\n",
    "\n",
    "    # ---- main API ----\n",
    "\n",
    "    def process_sentences(self, sentences: Sequence[str]) -> None:\n",
    "        for idx, sent in enumerate(sentences):\n",
    "            triples = self.triple_extractor(sent, idx)\n",
    "            for h, r, t in triples:\n",
    "                self._process_triple(sent, h, r, t)\n",
    "\n",
    "    def _process_triple(self, sentence: str, head: str, relation: str, tail: str) -> None:\n",
    "        emb = self.embedding_fn(head, relation, tail, sentence)\n",
    "        t1 = self.type_fn(head)\n",
    "        t2 = self.type_fn(tail)\n",
    "        type_pair: TypePair = (t1, t2)\n",
    "        self._register_type_pair(type_pair)\n",
    "\n",
    "        # Compute best cluster by cost\n",
    "        best_idx: Optional[int] = None\n",
    "        best_cost: float = float(\"inf\")\n",
    "\n",
    "        for idx, cluster in enumerate(self.clusters):\n",
    "            d_sem = cluster.semantic_distortion(emb)\n",
    "            d_type = cluster.type_distortion(type_pair, self.all_type_pairs_ordered)\n",
    "            cost = self.w_sem * d_sem + self.w_type * d_type\n",
    "            if cost < best_cost:\n",
    "                best_cost = cost\n",
    "                best_idx = idx\n",
    "\n",
    "        if best_idx is not None and best_cost < self.lambda_new:\n",
    "            cluster = self.clusters[best_idx]\n",
    "            cluster.update(head, relation, tail, emb, type_pair)\n",
    "            cid = cluster.cluster_id\n",
    "        else:\n",
    "            cid = len(self.clusters)\n",
    "            new_cluster = RelationCluster.from_first_example(\n",
    "                cluster_id=cid,\n",
    "                head=head,\n",
    "                relation=relation,\n",
    "                tail=tail,\n",
    "                emb=emb,\n",
    "                type_pair=type_pair,\n",
    "            )\n",
    "            self.clusters.append(new_cluster)\n",
    "\n",
    "        # append fact with its assigned cluster\n",
    "        self.fact_list.append((head, relation, tail, cid))\n",
    "\n",
    "    # ---- inspection helpers ----\n",
    "\n",
    "    def get_clusters_summary(self) -> List[Dict]:\n",
    "        summaries = []\n",
    "        for c in self.clusters:\n",
    "            summaries.append(\n",
    "                {\n",
    "                    \"cluster_id\": c.cluster_id,\n",
    "                    \"surface_relations\": sorted(c.surface_relations),\n",
    "                    \"n_triples\": c.count,\n",
    "                    \"canonical_type_pair\": c.canonical_type_pair(),\n",
    "                    \"type_counts\": dict(c.type_counts),\n",
    "                }\n",
    "            )\n",
    "        return summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7447f128",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Tiny toy example: you will replace this with your real IE + embeddings.\n",
    "    sentences = [\n",
    "        \"Alice works at AcmeCorp.\",\n",
    "        \"AcmeCorp employs Alice.\",\n",
    "        \"Bob works at AcmeCorp.\",\n",
    "        \"Carol works at InnoTech.\",\n",
    "        \"InnoTech employs Carol.\",\n",
    "    ]\n",
    "\n",
    "    def toy_triple_extractor(sentence: str, idx: int) -> List[Triple]:\n",
    "        s = sentence.lower()\n",
    "        if \"works at\" in s:\n",
    "            parts = sentence.split()\n",
    "            # Very hacky, just to demo the pipeline\n",
    "            subj = parts[0]\n",
    "            obj = parts[-1].strip(\".\")\n",
    "            return [(subj, \"works_at\", obj)]\n",
    "        if \"employs\" in s:\n",
    "            parts = sentence.split()\n",
    "            subj = parts[0]\n",
    "            obj = parts[-1].strip(\".\")\n",
    "            return [(subj, \"employs\", obj)]\n",
    "        return []\n",
    "\n",
    "    # Use random embeddings as placeholder; replace with OpenAI embeddings\n",
    "    clusterer = OnlineRelationClusterer(\n",
    "        embedding_fn=embedding_fn,\n",
    "        triple_extractor=toy_triple_extractor,\n",
    "        type_fn=type_function,\n",
    "        w_sem=0.5,\n",
    "        w_type=1.0,\n",
    "        lambda_new=3.0,\n",
    "    )\n",
    "\n",
    "    clusterer.process_sentences(sentences)\n",
    "\n",
    "    print(\"=== Relation Clusters ===\")\n",
    "    for summary in clusterer.get_clusters_summary():\n",
    "        print(summary)\n",
    "\n",
    "    # Learn pragmatic equivalence from the current graph\n",
    "    learner = PragmaticEquivalenceLearner(mi_threshold=0.3, min_shared_pairs=1)\n",
    "    learner.ingest(clusterer.clusters, clusterer.fact_list)\n",
    "    learner.compute_equivalences()\n",
    "\n",
    "    print(\"\\n=== Equivalence Classes ===\")\n",
    "    for cid, eq in learner.equivalence_classes.items():\n",
    "        print(f\"Cluster {cid} equivalent to {sorted(eq)}\")\n",
    "    print(\"\\n=== Inverse Map ===\")\n",
    "    for cid, inv in learner.inverse_map.items():\n",
    "        print(f\"Cluster {cid} inverse of {inv}\")\n",
    "\n",
    "    # Redundancy checker\n",
    "    red = PragmaticRedundancyChecker(learner)\n",
    "    # Seed with existing facts\n",
    "    for (h, r, t, cid) in clusterer.fact_list:\n",
    "        red.add_fact(h, cid, t)\n",
    "\n",
    "    print(\"\\n=== Test redundancy on new triple: AcmeCorp employs Bob ===\")\n",
    "    new_triple = (\"AcmeCorp\", \"employs\", \"Bob\")\n",
    "    triple_type = (type_function(new_triple[0]), type_function(new_triple[2]))\n",
    "    # In a real system you'd pass this through the clusterer to get cid\n",
    "    emb = embedding_fn(*new_triple, triple_type)\n",
    "    # Fake: pick the cluster whose mean is closest (for demo)\n",
    "    best_idx = None\n",
    "    best_dist = float(\"inf\")\n",
    "    for c in clusterer.clusters:\n",
    "        d = c.semantic_distortion(emb)\n",
    "        if d < best_dist:\n",
    "            best_dist = d\n",
    "            best_idx = c.cluster_id\n",
    "\n",
    "    cid_new = best_idx\n",
    "    h, r, t = new_triple\n",
    "    if cid_new is None or red.is_redundant(h, cid_new, t):\n",
    "        print(\"→ This triple is considered REDUNDANT.\")\n",
    "    else:\n",
    "        print(\"→ This triple is NEW information.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76502ea2",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "keys must be str, int, float, bool or None, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# json.dumps(dct)  # This will raise a TypeError\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdct\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/graph_of/lib/python3.11/json/__init__.py:238\u001b[39m, in \u001b[36mdumps\u001b[39m\u001b[34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[39m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    233\u001b[39m     \u001b[38;5;28mcls\u001b[39m = JSONEncoder\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m    \u001b[49m\u001b[43mskipkeys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheck_circular\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheck_circular\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseparators\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseparators\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/graph_of/lib/python3.11/json/encoder.py:200\u001b[39m, in \u001b[36mJSONEncoder.encode\u001b[39m\u001b[34m(self, o)\u001b[39m\n\u001b[32m    196\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m encode_basestring(o)\n\u001b[32m    197\u001b[39m \u001b[38;5;66;03m# This doesn't pass the iterator directly to ''.join() because the\u001b[39;00m\n\u001b[32m    198\u001b[39m \u001b[38;5;66;03m# exceptions aren't as detailed.  The list call should be roughly\u001b[39;00m\n\u001b[32m    199\u001b[39m \u001b[38;5;66;03m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m chunks = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_one_shot\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunks, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[32m    202\u001b[39m     chunks = \u001b[38;5;28mlist\u001b[39m(chunks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/graph_of/lib/python3.11/json/encoder.py:258\u001b[39m, in \u001b[36mJSONEncoder.iterencode\u001b[39m\u001b[34m(self, o, _one_shot)\u001b[39m\n\u001b[32m    253\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    254\u001b[39m     _iterencode = _make_iterencode(\n\u001b[32m    255\u001b[39m         markers, \u001b[38;5;28mself\u001b[39m.default, _encoder, \u001b[38;5;28mself\u001b[39m.indent, floatstr,\n\u001b[32m    256\u001b[39m         \u001b[38;5;28mself\u001b[39m.key_separator, \u001b[38;5;28mself\u001b[39m.item_separator, \u001b[38;5;28mself\u001b[39m.sort_keys,\n\u001b[32m    257\u001b[39m         \u001b[38;5;28mself\u001b[39m.skipkeys, _one_shot)\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_iterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: keys must be str, int, float, bool or None, not tuple"
     ]
    }
   ],
   "source": [
    "dct = {(\"key1\", \"key2\"): 42}\n",
    "import json\n",
    "\n",
    "# json.dumps(dct)  # This will raise a TypeError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22176890",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph_of",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
