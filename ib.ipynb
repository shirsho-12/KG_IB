{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90a56795",
   "metadata": {},
   "source": [
    "## Semantic Relation Clustering - IB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d157f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Callable, Dict, Iterable, List, Optional, Sequence, Tuple, Set\n",
    "import math\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pathlib import Path\n",
    "import openai\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "load_dotenv()\n",
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "OPENROUTER_BASE_URL = os.getenv(\"OPENROUTER_BASE_URL\", \"https://openrouter.ai/api/v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b7b790be",
   "metadata": {},
   "outputs": [],
   "source": [
    "Triple = Tuple[str, str, str]          # (head, relation, tail)\n",
    "TypePair = Tuple[str, str]             # (type(head), type(tail))\n",
    "EmbeddingFn = Callable[[str, str, str, str], np.ndarray]\n",
    "TypeFn = Callable[[str], str]\n",
    "TripleExtractorFn = Callable[[str, int], List[Triple]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3947e72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_path = Path.cwd() / \"output\" / \"webnlg\" / \"triplets.txt\"\n",
    "data_path = Path.cwd() / \"data\" / \"webnlg.txt\"\n",
    "\n",
    "triplets_text = triplet_path.read_text().splitlines()\n",
    "\n",
    "import ast\n",
    "all_triplets = [ast.literal_eval(line) for line in triplets_text]\n",
    "\n",
    "from model.openai_model import OpenAIModel\n",
    "from agent.core_agent import Agent\n",
    "\n",
    "model = OpenAIModel(\n",
    "    model_name=\"openai/gpt-4o-mini\",\n",
    "    base_url=OPENROUTER_BASE_URL,\n",
    "    api_key=OPENROUTER_API_KEY,\n",
    "    temperature=0.0\n",
    ")\n",
    "type_function_prompt = \"\"\"\n",
    "You are an expert at classifying entities into types. Given an entity, return its type in one or two words. Be concise and specific.\n",
    "Examples:\n",
    "- \"Barack Obama\" -> \"Person\"\n",
    "- \"New York City\" -> \"Location\"\n",
    "\n",
    "Only return the type without any additional explanation.\n",
    "Input: \"{entity}\"\n",
    "Output:\n",
    "\"\"\"\n",
    "\n",
    "type_function_agent = Agent(\n",
    "    llm=model,\n",
    "    prompt=type_function_prompt\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a793ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from typing import Union\n",
    "\n",
    "\n",
    "def default_triple_extractor(sentence: str, idx=0) -> List[Triple]:\n",
    "    \"\"\"\n",
    "    Stub: Extract (head, relation, tail) triples from a sentence.\n",
    "    Replace with your actual IE model (OpenIE, SRL, custom, etc.).\n",
    "\n",
    "    For now, returns [] so nothing happens unless you swap it out.\n",
    "    \"\"\"\n",
    "    return all_triplets[idx]\n",
    "\n",
    "\n",
    "def type_function(entity: str) -> str:\n",
    "    \"\"\"\n",
    "    GPT-based type function.\n",
    "    \"\"\"\n",
    "    return type_function_agent.run({\"entity\": entity}).strip().upper()\n",
    "\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    api_key=OPENROUTER_API_KEY,\n",
    "    base_url=OPENROUTER_BASE_URL\n",
    ")\n",
    "\n",
    "\n",
    "def embedding_fn(head: str, relation: str, tail: str, triple_type: Union[str, Tuple[str, str]]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    GPT-based embedding function.\n",
    "    \"\"\"\n",
    "    if isinstance(triple_type, tuple):\n",
    "        triple_type = f\"{triple_type[0]}->{triple_type[1]}\"\n",
    "    text = f\"{head} {relation} {tail} [{triple_type}]\"\n",
    "    response = client.embeddings.create(\n",
    "        input=[text],\n",
    "        model=\"text-embedding-3-small\"\n",
    "    )\n",
    "    embeddings = [\n",
    "        torch.tensor(data_point.embedding) for data_point in response.data\n",
    "        ]\n",
    "    return torch.stack(embeddings).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fcac2d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class RelationCluster:\n",
    "    \"\"\"\n",
    "    Represents an induced relation type.\n",
    "\n",
    "    Maintains:\n",
    "    - mean embedding μ\n",
    "    - diagonal variance estimate σ^2 (via online Welford updates)\n",
    "    - multinomial over argument type pairs (type(head), type(tail))\n",
    "    - set of surface relation strings that landed here\n",
    "    \"\"\"\n",
    "    # running stats for embeddings\n",
    "    mean: np.ndarray\n",
    "    var_diag: np.ndarray\n",
    "    count: int\n",
    "\n",
    "    # argument-type distribution\n",
    "    type_counts: Dict[TypePair, int] = field(default_factory=dict)\n",
    "\n",
    "    # surface labels\n",
    "    surface_relations: set = field(default_factory=set)\n",
    "\n",
    "    # small constant for numerical stability\n",
    "    eps: float = 1e-6\n",
    "\n",
    "    @classmethod\n",
    "    def from_first_example(\n",
    "        cls,\n",
    "        head: str,\n",
    "        relation: str,\n",
    "        tail: str,\n",
    "        emb: np.ndarray,\n",
    "        type_pair: TypePair,\n",
    "    ) -> \"RelationCluster\":\n",
    "        return cls(\n",
    "            mean=emb.copy(),\n",
    "            var_diag=np.ones_like(emb, dtype=np.float32),  # initial variance guess\n",
    "            count=1,\n",
    "            type_counts={type_pair: 1},\n",
    "            surface_relations={relation},\n",
    "        )\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        head: str,\n",
    "        relation: str,\n",
    "        tail: str,\n",
    "        emb: np.ndarray,\n",
    "        type_pair: TypePair,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Online update of:\n",
    "        - mean and diag variance (Welford-like per-dimension)\n",
    "        - type_counts\n",
    "        - surface_relations\n",
    "        \"\"\"\n",
    "        self.surface_relations.add(relation)\n",
    "\n",
    "        # Welford update for diagonal covariance\n",
    "        self.count += 1\n",
    "        delta = emb - self.mean\n",
    "        # update mean\n",
    "        self.mean += delta / float(self.count)\n",
    "        # recompute delta to new mean\n",
    "        delta2 = emb - self.mean\n",
    "        # online update of diagonal variance estimate\n",
    "        # M2_diag = (count-1)*var_diag; var_new = M2_new / (count-1)\n",
    "        M2_diag = self.var_diag * (self.count - 2)  # previous count-1 = current-2\n",
    "        M2_diag += delta * delta2\n",
    "        if self.count > 1:\n",
    "            self.var_diag = M2_diag / float(self.count - 1)\n",
    "        else:\n",
    "            self.var_diag = np.ones_like(self.mean, dtype=np.float32)\n",
    "\n",
    "        # keep some minimum variance\n",
    "        self.var_diag = np.maximum(self.var_diag, self.eps)\n",
    "\n",
    "        # update type distribution\n",
    "        self.type_counts[type_pair] = self.type_counts.get(type_pair, 0) + 1\n",
    "\n",
    "    # ---------- type distortion (argument-role compatibility) ----------\n",
    "\n",
    "    def type_probability(\n",
    "        self,\n",
    "        type_pair: TypePair,\n",
    "        all_type_pairs: Sequence[TypePair],\n",
    "        alpha_smooth: float = 1.0,\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Laplace-smoothed probability P(type_pair | cluster).\n",
    "        \"\"\"\n",
    "        K = len(all_type_pairs)\n",
    "        total = sum(self.type_counts.values())\n",
    "        count = self.type_counts.get(type_pair, 0)\n",
    "        return (count + alpha_smooth) / (total + alpha_smooth * K)\n",
    "\n",
    "    def type_distortion(\n",
    "        self,\n",
    "        type_pair: TypePair,\n",
    "        all_type_pairs: Sequence[TypePair],\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        D_type = -log2 P(type_pair | cluster).\n",
    "        Lower is better.\n",
    "        \"\"\"\n",
    "        p = self.type_probability(type_pair, all_type_pairs)\n",
    "        return -math.log(p + 1e-12, 2.0)  # bits\n",
    "\n",
    "    # ---------- semantic distortion (embedding distance / KL-ish) ----------\n",
    "\n",
    "    def semantic_distortion(self, emb: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Mahalanobis-like distance between emb and cluster Gaussian:\n",
    "\n",
    "        D_sem ≈ (x - μ)^T diag(1/σ^2) (x - μ)\n",
    "\n",
    "        This is proportional to KL(N(μ,σ^2) || N(x, σ0^2 I)) under an isotropic\n",
    "        assumption, so it's a reasonable proxy for semantic KL.\n",
    "        \"\"\"\n",
    "        diff = emb - self.mean\n",
    "        inv_var = 1.0 / (self.var_diag + self.eps)\n",
    "        return float(np.sum(diff * diff * inv_var))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f3189d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Online relation clusterer\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "class RelationClusterer:\n",
    "    \"\"\"\n",
    "    Online clustering of relation instances into induced relation types.\n",
    "\n",
    "    For each triple (h, r, t) in a sentence:\n",
    "      - Compute an embedding embedding_fn(h, r, t, sentence)\n",
    "      - Infer types T1 = type_fn(h), T2 = type_fn(t)\n",
    "      - For each cluster, compute:\n",
    "            cost = w_sem * D_sem + w_type * D_type\n",
    "        where:\n",
    "            D_sem  = semantic_distortion(embedding)\n",
    "            D_type = -log P((T1,T2) | cluster)\n",
    "      - If min_cluster_cost < lambda_new:\n",
    "            assign to that cluster and update its stats\n",
    "        else:\n",
    "            create a new cluster\n",
    "\n",
    "    This is a greedy, streaming approximation to an IB-style objective:\n",
    "        minimize E[cost] + lambda_new * (#clusters)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_fn: EmbeddingFn,\n",
    "        triple_extractor: TripleExtractorFn = default_triple_extractor,\n",
    "        type_fn: TypeFn = type_function,\n",
    "        w_sem: float = 0.5,\n",
    "        w_type: float = 1.0,\n",
    "        lambda_new: float = 3.0,\n",
    "    ) -> None:\n",
    "        self.embedding_fn = embedding_fn\n",
    "        self.triple_extractor = triple_extractor\n",
    "        self.type_fn = type_fn\n",
    "\n",
    "        self.w_sem = w_sem\n",
    "        self.w_type = w_type\n",
    "        self.lambda_new = lambda_new\n",
    "\n",
    "        self.clusters: List[RelationCluster] = []\n",
    "        self.all_type_pairs: Dict[TypePair, None] = {}  # use as ordered set\n",
    "\n",
    "    # ---------- main entry point ----------\n",
    "\n",
    "    def process_sentences(self, sentences: Sequence[str]) -> None:\n",
    "        \"\"\"\n",
    "        Process a sequence of sentences, updating clusters online.\n",
    "        \"\"\"\n",
    "        for idx, sent in tqdm(enumerate(sentences), total=len(sentences)):\n",
    "            triples = self.triple_extractor(sent, idx)\n",
    "            for h, r, t in triples:\n",
    "                self._process_triple(sent, h, r, t)\n",
    "\n",
    "    # ---------- internals ----------\n",
    "\n",
    "    def _process_triple(self, sentence: str, head: str, relation: str, tail: str) -> None:\n",
    "        # 1) embedding\n",
    "        emb = self.embedding_fn(head, relation, tail, sentence)\n",
    "\n",
    "        # 2) argument types\n",
    "        t1 = self.type_fn(head)\n",
    "        t2 = self.type_fn(tail)\n",
    "        type_pair: TypePair = (t1, t2)\n",
    "        if type_pair not in self.all_type_pairs:\n",
    "            self.all_type_pairs[type_pair] = None\n",
    "\n",
    "        # 3) compute cost for each existing cluster\n",
    "        best_idx: Optional[int] = None\n",
    "        best_cost: float = float(\"inf\")\n",
    "        best_Dsem: Optional[float] = None\n",
    "        best_Dtype: Optional[float] = None\n",
    "\n",
    "        type_pair_list = list(self.all_type_pairs.keys())\n",
    "\n",
    "        for idx, cluster in enumerate(self.clusters):\n",
    "            d_sem = cluster.semantic_distortion(emb)\n",
    "            d_type = cluster.type_distortion(type_pair, type_pair_list)\n",
    "            cost = self.w_sem * d_sem + self.w_type * d_type\n",
    "\n",
    "            if cost < best_cost:\n",
    "                best_cost = cost\n",
    "                best_idx = idx\n",
    "                best_Dsem = d_sem\n",
    "                best_Dtype = d_type\n",
    "\n",
    "        # 4) decide: assign vs create new\n",
    "        if best_idx is not None and best_cost < self.lambda_new:\n",
    "            cluster = self.clusters[best_idx]\n",
    "            cluster.update(head, relation, tail, emb, type_pair)\n",
    "            # could log best_Dsem/best_Dtype here if desired\n",
    "        else:\n",
    "            new_cluster = RelationCluster.from_first_example(\n",
    "                head=head,\n",
    "                relation=relation,\n",
    "                tail=tail,\n",
    "                emb=emb,\n",
    "                type_pair=type_pair,\n",
    "            )\n",
    "            self.clusters.append(new_cluster)\n",
    "\n",
    "    # ---------- convenience methods / inspection ----------\n",
    "\n",
    "    def get_clusters_summary(self) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Return a light-weight summary of clusters for inspection.\n",
    "        \"\"\"\n",
    "        summaries = []\n",
    "        for idx, c in enumerate(self.clusters):\n",
    "            summaries.append(\n",
    "                {\n",
    "                    \"cluster_id\": idx,\n",
    "                    \"surface_relations\": sorted(c.surface_relations),\n",
    "                    \"n_triples\": c.count,\n",
    "                    \"type_counts\": dict(c.type_counts),\n",
    "                }\n",
    "            )\n",
    "        return summaries\n",
    "\n",
    "    def print_clusters(self, max_width: int = 120) -> None:\n",
    "        \"\"\"\n",
    "        Pretty-print clusters in a compact way.\n",
    "        \"\"\"\n",
    "        import textwrap\n",
    "        for summary in self.get_clusters_summary():\n",
    "            s = textwrap.shorten(str(summary), width=max_width)\n",
    "            print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7744dcf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1165 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 597/1165 [1:26:18<1:24:53,  8.97s/it]"
     ]
    }
   ],
   "source": [
    "sentences = Path(data_path).read_text().splitlines()\n",
    "\n",
    "clusterer = RelationClusterer(\n",
    "    embedding_fn=embedding_fn,\n",
    "    triple_extractor=default_triple_extractor,\n",
    "    type_fn=type_function,\n",
    "    w_sem=0.5,\n",
    "    w_type=1.0,\n",
    "    lambda_new=3.0,\n",
    ")\n",
    "\n",
    "clusterer.process_sentences(sentences)\n",
    "clusterer.print_clusters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b253714b",
   "metadata": {},
   "source": [
    "## Online Clusterer - Pragmatic Equivalence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbb8626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MI computation utility\n",
    "def mutual_information_binary(N11, N10, N01, N00):\n",
    "    \"\"\"\n",
    "    Compute mutual information I(X;Y) for binary events:\n",
    "        X = \"edge of cluster c1 under mapping M\"\n",
    "        Y = \"edge of cluster c2 under mapping M\"\n",
    "\n",
    "    Where:\n",
    "      N11 = count(X=1, Y=1)\n",
    "      N10 = count(X=1, Y=0)\n",
    "      N01 = count(X=0, Y=1)\n",
    "      N00 = count(X=0, Y=0)\n",
    "\n",
    "    Returns MI in bits.\n",
    "    \"\"\"\n",
    "    N = N11 + N10 + N01 + N00\n",
    "    if N == 0:\n",
    "        return 0.0\n",
    "\n",
    "    P11 = N11 / N\n",
    "    P10 = N10 / N\n",
    "    P01 = N01 / N\n",
    "    P00 = N00 / N\n",
    "\n",
    "    PX1 = P11 + P10\n",
    "    PY1 = P11 + P01\n",
    "    PX0 = 1 - PX1\n",
    "    PY0 = 1 - PY1\n",
    "\n",
    "    MI = 0.0\n",
    "\n",
    "    def add_term(Pxy, Px, Py):\n",
    "        return Pxy * math.log2(Pxy / (Px * Py)) if Pxy > 0 else 0.0\n",
    "\n",
    "    MI += add_term(P11, PX1, PY1)\n",
    "    MI += add_term(P10, PX1, PY0)\n",
    "    MI += add_term(P01, PX0, PY1)\n",
    "    MI += add_term(P00, PX0, PY0)\n",
    "\n",
    "    return MI\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RelationClusterView:\n",
    "    \"\"\"\n",
    "    Lightweight extensional view:\n",
    "    - cluster_id\n",
    "    - canonical type_pair\n",
    "    - edges: set of (h, t)\n",
    "    \"\"\"\n",
    "    cluster_id: int\n",
    "    type_pair: TypePair\n",
    "    edges: Set[Tuple[str, str]] = field(default_factory=set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e5f5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RelationCluster:\n",
    "    \"\"\"\n",
    "    Represents an induced relation type.\n",
    "\n",
    "    Maintains:\n",
    "    - mean embedding μ and diagonal variance σ^2 (online)\n",
    "    - type_counts over (type(head), type(tail))\n",
    "    - surface_relations: set of raw relation strings\n",
    "    - cluster_id: int assigned by clusterer\n",
    "    \"\"\"\n",
    "    cluster_id: int\n",
    "    mean: np.ndarray\n",
    "    var_diag: np.ndarray\n",
    "    count: int\n",
    "\n",
    "    type_counts: Dict[TypePair, int] = field(default_factory=dict)\n",
    "    surface_relations: Set[str] = field(default_factory=set)\n",
    "    eps: float = 1e-6\n",
    "\n",
    "    @classmethod\n",
    "    def from_first_example(\n",
    "        cls,\n",
    "        cluster_id: int,\n",
    "        head: str,\n",
    "        relation: str,\n",
    "        tail: str,\n",
    "        emb: np.ndarray,\n",
    "        type_pair: TypePair,\n",
    "    ) -> \"RelationCluster\":\n",
    "        return cls(\n",
    "            cluster_id=cluster_id,\n",
    "            mean=emb.copy(),\n",
    "            var_diag=np.ones_like(emb, dtype=np.float32),\n",
    "            count=1,\n",
    "            type_counts={type_pair: 1},\n",
    "            surface_relations={relation},\n",
    "        )\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        head: str,\n",
    "        relation: str,\n",
    "        tail: str,\n",
    "        emb: np.ndarray,\n",
    "        type_pair: TypePair,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Online update of:\n",
    "        - mean & diag variance via Welford per-dimension\n",
    "        - type_counts\n",
    "        - surface_relations\n",
    "        \"\"\"\n",
    "        self.surface_relations.add(relation)\n",
    "\n",
    "        self.count += 1\n",
    "        delta = emb - self.mean\n",
    "        self.mean += delta / float(self.count)\n",
    "        delta2 = emb - self.mean\n",
    "\n",
    "        # Online diag variance: M2_diag / (count-1) = var_diag\n",
    "        M2_diag = self.var_diag * (self.count - 2)  # previous (count-1) = current-2\n",
    "        M2_diag += delta * delta2\n",
    "        if self.count > 1:\n",
    "            self.var_diag = M2_diag / float(self.count - 1)\n",
    "        else:\n",
    "            self.var_diag = np.ones_like(self.mean, dtype=np.float32)\n",
    "\n",
    "        self.var_diag = np.maximum(self.var_diag, self.eps)\n",
    "\n",
    "        self.type_counts[type_pair] = self.type_counts.get(type_pair, 0) + 1\n",
    "\n",
    "    # ---- type distortion (argument-role compatibility) ----\n",
    "\n",
    "    def canonical_type_pair(self) -> TypePair:\n",
    "        \"\"\"\n",
    "        Return the most frequent type_pair (for MI / equivalence).\n",
    "        \"\"\"\n",
    "        if not self.type_counts:\n",
    "            return (\"UNKNOWN\", \"UNKNOWN\")\n",
    "        return max(self.type_counts.items(), key=lambda kv: kv[1])[0]\n",
    "\n",
    "    def type_probability(\n",
    "        self,\n",
    "        type_pair: TypePair,\n",
    "        all_type_pairs: Sequence[TypePair],\n",
    "        alpha_smooth: float = 1.0,\n",
    "    ) -> float:\n",
    "        K = len(all_type_pairs)\n",
    "        total = sum(self.type_counts.values())\n",
    "        count = self.type_counts.get(type_pair, 0)\n",
    "        return (count + alpha_smooth) / (total + alpha_smooth * K)\n",
    "\n",
    "    def type_distortion(\n",
    "        self,\n",
    "        type_pair: TypePair,\n",
    "        all_type_pairs: Sequence[TypePair],\n",
    "    ) -> float:\n",
    "        \"\"\"D_type = -log2 P(type_pair | cluster). Lower is better.\"\"\"\n",
    "        p = self.type_probability(type_pair, all_type_pairs)\n",
    "        return -math.log(p + 1e-12, 2.0)\n",
    "\n",
    "    # ---- semantic distortion (embedding distance / KL-ish) ----\n",
    "\n",
    "    def semantic_distortion(self, emb: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Mahalanobis-like distance:\n",
    "        D_sem ≈ (x - μ)^T diag(1/σ^2) (x - μ)\n",
    "        \"\"\"\n",
    "        diff = emb - self.mean\n",
    "        inv_var = 1.0 / (self.var_diag + self.eps)\n",
    "        return float(np.sum(diff * diff * inv_var))\n",
    "\n",
    "\n",
    "class PragmaticEquivalenceLearner:\n",
    "    \"\"\"\n",
    "    Learns pragmatic equivalence (same-direction or inverse) between\n",
    "    induced relation clusters using extensional mutual information.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mi_threshold: float = 0.25, min_shared_pairs: int = 2) -> None:\n",
    "        self.mi_threshold = mi_threshold\n",
    "        self.min_shared_pairs = min_shared_pairs\n",
    "\n",
    "        self.views: Dict[int, RelationClusterView] = {}\n",
    "        self.equivalence_classes: Dict[int, Set[int]] = defaultdict(set)\n",
    "        self.inverse_map: Dict[int, int] = {}  # cluster_id -> inverse_cluster_id\n",
    "\n",
    "    def ingest(self, clusters: List[RelationCluster], fact_list: List[Tuple[str, str, str, int]]) -> None:\n",
    "        \"\"\"\n",
    "        Build RelationClusterView objects from clusters and facts.\n",
    "        \"\"\"\n",
    "        for c in clusters:\n",
    "            self.views[c.cluster_id] = RelationClusterView(\n",
    "                cluster_id=c.cluster_id,\n",
    "                type_pair=c.canonical_type_pair(),\n",
    "            )\n",
    "\n",
    "        for (h, r, t, cid) in fact_list:\n",
    "            if cid in self.views:\n",
    "                self.views[cid].edges.add((h, t))\n",
    "\n",
    "    def compute_equivalences(self) -> None:\n",
    "        ids = sorted(self.views.keys())\n",
    "        for i in range(len(ids)):\n",
    "            for j in range(i + 1, len(ids)):\n",
    "                v1 = self.views[ids[i]]\n",
    "                v2 = self.views[ids[j]]\n",
    "\n",
    "                same_dir = (v1.type_pair == v2.type_pair)\n",
    "                inverse_dir = (v1.type_pair == (v2.type_pair[1], v2.type_pair[0]))\n",
    "\n",
    "                if not (same_dir or inverse_dir):\n",
    "                    continue\n",
    "\n",
    "                if same_dir:\n",
    "                    MI = self._mi_same_direction(v1, v2)\n",
    "                    direction = \"same\"\n",
    "                else:\n",
    "                    MI = self._mi_inverse_direction(v1, v2)\n",
    "                    direction = \"inverse\"\n",
    "\n",
    "                if MI <= 0:\n",
    "                    continue\n",
    "\n",
    "                H1 = self._binary_entropy(v1.edges)\n",
    "                H2 = self._binary_entropy(v2.edges)\n",
    "                denom = min(H1, H2) if min(H1, H2) > 0 else 1.0\n",
    "                NMI = MI / denom\n",
    "\n",
    "                if NMI >= self.mi_threshold:\n",
    "                    self.equivalence_classes[v1.cluster_id].add(v2.cluster_id)\n",
    "                    self.equivalence_classes[v2.cluster_id].add(v1.cluster_id)\n",
    "                    if direction == \"inverse\":\n",
    "                        self.inverse_map[v1.cluster_id] = v2.cluster_id\n",
    "                        self.inverse_map[v2.cluster_id] = v1.cluster_id\n",
    "\n",
    "    def _mi_same_direction(self, v1: RelationClusterView, v2: RelationClusterView) -> float:\n",
    "        all_pairs = v1.edges.union(v2.edges)\n",
    "        if len(all_pairs) < self.min_shared_pairs:\n",
    "            return 0.0\n",
    "        e1 = v1.edges\n",
    "        e2 = v2.edges\n",
    "\n",
    "        N11 = sum(1 for p in all_pairs if (p in e1 and p in e2))\n",
    "        N10 = sum(1 for p in all_pairs if (p in e1 and p not in e2))\n",
    "        N01 = sum(1 for p in all_pairs if (p in e2 and p not in e1))\n",
    "        N00 = len(all_pairs) - (N11 + N10 + N01)\n",
    "        return mutual_information_binary(N11, N10, N01, N00)\n",
    "\n",
    "    def _mi_inverse_direction(self, v1: RelationClusterView, v2: RelationClusterView) -> float:\n",
    "        e1 = v1.edges\n",
    "        e2_swapped = {(t, h) for (h, t) in v2.edges}\n",
    "        all_pairs = e1.union(e2_swapped)\n",
    "        if len(all_pairs) < self.min_shared_pairs:\n",
    "            return 0.0\n",
    "\n",
    "        N11 = sum(1 for p in all_pairs if (p in e1 and p in e2_swapped))\n",
    "        N10 = sum(1 for p in all_pairs if (p in e1 and p not in e2_swapped))\n",
    "        N01 = sum(1 for p in all_pairs if (p in e2_swapped and p not in e1))\n",
    "        N00 = len(all_pairs) - (N11 + N10 + N01)\n",
    "        return mutual_information_binary(N11, N10, N01, N00)\n",
    "\n",
    "    @staticmethod\n",
    "    def _binary_entropy(edge_set: Set[Tuple[str, str]]) -> float:\n",
    "        N = len(edge_set)\n",
    "        if N == 0:\n",
    "            return 0.0\n",
    "        P1 = N / (N + 1e-12)\n",
    "        P0 = 1 - P1\n",
    "        return -(P1 * math.log2(P1 + 1e-12) + P0 * math.log2(P0 + 1e-12))\n",
    "\n",
    "\n",
    "############################################################\n",
    "# Redundancy checker based on equivalence classes\n",
    "############################################################\n",
    "\n",
    "class PragmaticRedundancyChecker:\n",
    "    \"\"\"\n",
    "    Uses learned pragmatic equivalence (same + inverse) to decide\n",
    "    whether a new triple (h, cid, t) is redundant.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, learner: PragmaticEquivalenceLearner) -> None:\n",
    "        self.learner = learner\n",
    "        self.forward_edges = defaultdict(set)   # (h, cid) -> set(t)\n",
    "        self.backward_edges = defaultdict(set)  # (t, cid) -> set(h)\n",
    "\n",
    "    def add_fact(self, h: str, cid: int, t: str) -> None:\n",
    "        self.forward_edges[(h, cid)].add(t)\n",
    "        self.backward_edges[(t, cid)].add(h)\n",
    "\n",
    "    def is_redundant(self, h: str, cid: int, t: str) -> bool:\n",
    "        # Direct same-cluster fact\n",
    "        if t in self.forward_edges.get((h, cid), set()):\n",
    "            return True\n",
    "\n",
    "        # Check equivalent clusters\n",
    "        eq_class = self.learner.equivalence_classes.get(cid, set())\n",
    "\n",
    "        for cid2 in eq_class:\n",
    "            # same direction\n",
    "            if t in self.forward_edges.get((h, cid2), set()):\n",
    "                return True\n",
    "\n",
    "            # inverse direction\n",
    "            inv = self.learner.inverse_map.get(cid2)\n",
    "            if inv is not None and inv == cid:\n",
    "                if h in self.backward_edges.get((t, cid2), set()):\n",
    "                    return True\n",
    "\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c74c305",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class OnlineRelationClusterer:\n",
    "    \"\"\"\n",
    "    Streaming clustering of relation instances into induced relation types.\n",
    "\n",
    "    For each triple (h,r,t) extracted from sentences:\n",
    "      - compute embedding\n",
    "      - infer types (T1,T2)\n",
    "      - cost(cluster) = w_sem * D_sem + w_type * D_type\n",
    "      - if min cost < lambda_new: assign to cluster, else new cluster.\n",
    "\n",
    "    Stores:\n",
    "      - clusters (RelationCluster)\n",
    "      - fact_list: (head, relation, tail, cluster_id) for all accepted facts\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_fn: EmbeddingFn,\n",
    "        triple_extractor: TripleExtractorFn = default_triple_extractor,\n",
    "        type_fn: TypeFn = type_function,\n",
    "        w_sem: float = 0.5,\n",
    "        w_type: float = 1.0,\n",
    "        lambda_new: float = 3.0,\n",
    "    ) -> None:\n",
    "        self.embedding_fn = embedding_fn\n",
    "        self.triple_extractor = triple_extractor\n",
    "        self.type_fn = type_fn\n",
    "\n",
    "        self.w_sem = w_sem\n",
    "        self.w_type = w_type\n",
    "        self.lambda_new = lambda_new\n",
    "\n",
    "        self.clusters: List[RelationCluster] = []\n",
    "        self.all_type_pairs_ordered: List[TypePair] = []\n",
    "        self._type_pair_index: Dict[TypePair, int] = {}\n",
    "        self.fact_list: List[Tuple[str, str, str, int]] = []  # (h, r, t, cluster_id)\n",
    "\n",
    "    # ---- type-pair registry ----\n",
    "\n",
    "    def _register_type_pair(self, tp: TypePair) -> None:\n",
    "        if tp not in self._type_pair_index:\n",
    "            self._type_pair_index[tp] = len(self.all_type_pairs_ordered)\n",
    "            self.all_type_pairs_ordered.append(tp)\n",
    "\n",
    "    # ---- main API ----\n",
    "\n",
    "    def process_sentences(self, sentences: Sequence[str]) -> None:\n",
    "        for idx, sent in enumerate(sentences):\n",
    "            triples = self.triple_extractor(sent, idx)\n",
    "            for h, r, t in triples:\n",
    "                self._process_triple(sent, h, r, t)\n",
    "\n",
    "    def _process_triple(self, sentence: str, head: str, relation: str, tail: str) -> None:\n",
    "        emb = self.embedding_fn(head, relation, tail, sentence)\n",
    "        t1 = self.type_fn(head)\n",
    "        t2 = self.type_fn(tail)\n",
    "        type_pair: TypePair = (t1, t2)\n",
    "        self._register_type_pair(type_pair)\n",
    "\n",
    "        # Compute best cluster by cost\n",
    "        best_idx: Optional[int] = None\n",
    "        best_cost: float = float(\"inf\")\n",
    "\n",
    "        for idx, cluster in enumerate(self.clusters):\n",
    "            d_sem = cluster.semantic_distortion(emb)\n",
    "            d_type = cluster.type_distortion(type_pair, self.all_type_pairs_ordered)\n",
    "            cost = self.w_sem * d_sem + self.w_type * d_type\n",
    "            if cost < best_cost:\n",
    "                best_cost = cost\n",
    "                best_idx = idx\n",
    "\n",
    "        if best_idx is not None and best_cost < self.lambda_new:\n",
    "            cluster = self.clusters[best_idx]\n",
    "            cluster.update(head, relation, tail, emb, type_pair)\n",
    "            cid = cluster.cluster_id\n",
    "        else:\n",
    "            cid = len(self.clusters)\n",
    "            new_cluster = RelationCluster.from_first_example(\n",
    "                cluster_id=cid,\n",
    "                head=head,\n",
    "                relation=relation,\n",
    "                tail=tail,\n",
    "                emb=emb,\n",
    "                type_pair=type_pair,\n",
    "            )\n",
    "            self.clusters.append(new_cluster)\n",
    "\n",
    "        # append fact with its assigned cluster\n",
    "        self.fact_list.append((head, relation, tail, cid))\n",
    "\n",
    "    # ---- inspection helpers ----\n",
    "\n",
    "    def get_clusters_summary(self) -> List[Dict]:\n",
    "        summaries = []\n",
    "        for c in self.clusters:\n",
    "            summaries.append(\n",
    "                {\n",
    "                    \"cluster_id\": c.cluster_id,\n",
    "                    \"surface_relations\": sorted(c.surface_relations),\n",
    "                    \"n_triples\": c.count,\n",
    "                    \"canonical_type_pair\": c.canonical_type_pair(),\n",
    "                    \"type_counts\": dict(c.type_counts),\n",
    "                }\n",
    "            )\n",
    "        return summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7447f128",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Tiny toy example: you will replace this with your real IE + embeddings.\n",
    "    sentences = [\n",
    "        \"Alice works at AcmeCorp.\",\n",
    "        \"AcmeCorp employs Alice.\",\n",
    "        \"Bob works at AcmeCorp.\",\n",
    "        \"Carol works at InnoTech.\",\n",
    "        \"InnoTech employs Carol.\",\n",
    "    ]\n",
    "\n",
    "    def toy_triple_extractor(sentence: str, idx: int) -> List[Triple]:\n",
    "        s = sentence.lower()\n",
    "        if \"works at\" in s:\n",
    "            parts = sentence.split()\n",
    "            # Very hacky, just to demo the pipeline\n",
    "            subj = parts[0]\n",
    "            obj = parts[-1].strip(\".\")\n",
    "            return [(subj, \"works_at\", obj)]\n",
    "        if \"employs\" in s:\n",
    "            parts = sentence.split()\n",
    "            subj = parts[0]\n",
    "            obj = parts[-1].strip(\".\")\n",
    "            return [(subj, \"employs\", obj)]\n",
    "        return []\n",
    "\n",
    "    # Use random embeddings as placeholder; replace with OpenAI embeddings\n",
    "    clusterer = OnlineRelationClusterer(\n",
    "        embedding_fn=embedding_fn,\n",
    "        triple_extractor=toy_triple_extractor,\n",
    "        type_fn=type_function,\n",
    "        w_sem=0.5,\n",
    "        w_type=1.0,\n",
    "        lambda_new=3.0,\n",
    "    )\n",
    "\n",
    "    clusterer.process_sentences(sentences)\n",
    "\n",
    "    print(\"=== Relation Clusters ===\")\n",
    "    for summary in clusterer.get_clusters_summary():\n",
    "        print(summary)\n",
    "\n",
    "    # Learn pragmatic equivalence from the current graph\n",
    "    learner = PragmaticEquivalenceLearner(mi_threshold=0.3, min_shared_pairs=1)\n",
    "    learner.ingest(clusterer.clusters, clusterer.fact_list)\n",
    "    learner.compute_equivalences()\n",
    "\n",
    "    print(\"\\n=== Equivalence Classes ===\")\n",
    "    for cid, eq in learner.equivalence_classes.items():\n",
    "        print(f\"Cluster {cid} equivalent to {sorted(eq)}\")\n",
    "    print(\"\\n=== Inverse Map ===\")\n",
    "    for cid, inv in learner.inverse_map.items():\n",
    "        print(f\"Cluster {cid} inverse of {inv}\")\n",
    "\n",
    "    # Redundancy checker\n",
    "    red = PragmaticRedundancyChecker(learner)\n",
    "    # Seed with existing facts\n",
    "    for (h, r, t, cid) in clusterer.fact_list:\n",
    "        red.add_fact(h, cid, t)\n",
    "\n",
    "    print(\"\\n=== Test redundancy on new triple: AcmeCorp employs Bob ===\")\n",
    "    new_triple = (\"AcmeCorp\", \"employs\", \"Bob\")\n",
    "    triple_type = (type_function(new_triple[0]), type_function(new_triple[2]))\n",
    "    # In a real system you'd pass this through the clusterer to get cid\n",
    "    emb = embedding_fn(*new_triple, triple_type)\n",
    "    # Fake: pick the cluster whose mean is closest (for demo)\n",
    "    best_idx = None\n",
    "    best_dist = float(\"inf\")\n",
    "    for c in clusterer.clusters:\n",
    "        d = c.semantic_distortion(emb)\n",
    "        if d < best_dist:\n",
    "            best_dist = d\n",
    "            best_idx = c.cluster_id\n",
    "\n",
    "    cid_new = best_idx\n",
    "    h, r, t = new_triple\n",
    "    if cid_new is None or red.is_redundant(h, cid_new, t):\n",
    "        print(\"→ This triple is considered REDUNDANT.\")\n",
    "    else:\n",
    "        print(\"→ This triple is NEW information.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph_of",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
